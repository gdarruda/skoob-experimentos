{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==0.24.2\n",
    "!pip install nltk==3.6.5\n",
    "!pip install seaborn==0.11.2\n",
    "!pip install gensim==4.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = '/tf/sa-experiments/corpus'\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    !tar xvzf corpus.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RSLPStemmer\n",
    "stemmer = RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = 745307\n",
    "validation_samples = 82811\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "corpus_dir = '/tf/sa-experiments/corpus/reviews'\n",
    "\n",
    "dataset_training = (preprocessing\n",
    "    .text_dataset_from_directory(corpus_dir,\n",
    "                                 validation_split=0.1,\n",
    "                                 subset='training',\n",
    "                                 shuffle=True,\n",
    "                                 batch_size=batch_size,\n",
    "                                 seed=seed)\n",
    ")\n",
    "\n",
    "class_names = dataset_training.class_names\n",
    "\n",
    "dataset_validation = (preprocessing\n",
    "    .text_dataset_from_directory(\n",
    "        corpus_dir,\n",
    "        validation_split=0.1,\n",
    "        subset='validation',\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        seed=seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np_dataset(dataset, vectorizer, fit_dict = True):\n",
    "\n",
    "    dataset_np = np.concatenate([\n",
    "        np.concatenate([x.reshape(x.shape[0], 1),\n",
    "                        y.reshape(x.shape[0], 1)], axis=1)\n",
    "                        for x, y in dataset.as_numpy_iterator()])\n",
    "    \n",
    "    \n",
    "    X = vectorizer.fit_transform(dataset_np[:,0]) if fit_dict else vectorizer.transform(dataset_np[:,0])\n",
    "    Y = dataset_np[:,1].astype(int)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "def plot_results(clf, X_validation, Y_validation):\n",
    "\n",
    "    Y_predict = clf.predict(X_validation)\n",
    "    fig, axs = plt.subplots(figsize=(15, 20), nrows=2, ncols=1)\n",
    "\n",
    "    map_classes = np.vectorize(lambda x: class_names[x])\n",
    "    \n",
    "    sns.heatmap(pd.DataFrame(confusion_matrix(map_classes(Y_predict), \n",
    "                                              map_classes(Y_validation), \n",
    "                                              normalize='true', \n",
    "                                              labels=class_names), \n",
    "                             columns=class_names, \n",
    "                             index=class_names), \n",
    "                annot=True,\n",
    "                cmap='Blues',\n",
    "                ax=axs[0])\n",
    "    \n",
    "    sns.heatmap(pd.DataFrame(confusion_matrix(map_classes(Y_predict), \n",
    "                                              map_classes(Y_validation),\n",
    "                                              labels=class_names), \n",
    "                             columns=class_names, \n",
    "                             index=class_names), \n",
    "                annot=True,\n",
    "                cmap='Blues',\n",
    "                ax=axs[1])\n",
    "\n",
    "    axs[0].set_title(f'{clf.__class__.__name__} | Accuracy : {round(accuracy_score(Y_validation, Y_predict), 2)}')\n",
    "    axs[1].set_title(f'{clf.__class__.__name__} | Accuracy : {accuracy_score(Y_validation, Y_predict, normalize=False)}')\n",
    "\n",
    "    fig.savefig(f'/tf/sa-experiments/{clf.__class__.__name__}.svg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def analyzer_stemming(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('portuguese'))\n",
    "\n",
    "X_training, Y_training = to_np_dataset(dataset_training, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_training, Y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation, Y_validation = to_np_dataset(dataset_validation, vectorizer, fit_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(clf, X_validation, Y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_training, Y_training = to_np_dataset(dataset_training, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_training, Y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation, Y_validation = to_np_dataset(dataset_validation, vectorizer, fit_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(clf, X_validation, Y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador SVM (Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "tokenizer = vectorizer.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "POOL = 12\n",
    "\n",
    "base_dir = 'sa-experiments/corpus'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(f'{base_dir}/embeddings/glove_s{EMBEDDING_DIM}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def to_embedding(tokens):\n",
    "    \n",
    "    hits = 0\n",
    "    embedding = np.zeros((EMBEDDING_DIM,1))\n",
    "    \n",
    "    for token in tokens:\n",
    "        \n",
    "        if embeddings_index.has_index_for(token):\n",
    "            hits+=1\n",
    "            embedding = embedding + embeddings_index[token].reshape(EMBEDDING_DIM,1)\n",
    "\n",
    "    return embedding / hits if hits > 0 else embedding\n",
    "\n",
    "def to_embedding_batch(batch):\n",
    "    reviews, classes = batch\n",
    "    embeddings =  np.concatenate([to_embedding(\n",
    "                                  tokenizer(review.decode()))\n",
    "                                  for review in reviews], axis=1).T\n",
    "    return np.concatenate([embeddings, \n",
    "                           classes.reshape(classes.shape[0], 1)], \n",
    "                           axis = 1)\n",
    "\n",
    "def split_dimensions(dataset_numpy):\n",
    "    \n",
    "    dataset_numpy = np.concatenate(dataset_numpy)\n",
    "\n",
    "    return (dataset_numpy[:,:EMBEDDING_DIM],\n",
    "            dataset_numpy[:,EMBEDDING_DIM].astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(POOL) as p:\n",
    "    dataset_training_numpy = p.map(to_embedding_batch, [batch for batch\n",
    "                                                        in dataset_training.as_numpy_iterator()])\n",
    "\n",
    "X_training, Y_training = split_dimensions(dataset_training_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(POOL) as p:\n",
    "    dataset_validation_numpy = p.map(to_embedding_batch, [batch for batch\n",
    "                                                         in dataset_validation.as_numpy_iterator()])\n",
    "\n",
    "X_validation, Y_validation = split_dimensions(dataset_validation_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_training, Y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(clf, X_validation, Y_validation)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7d30c5eb6ce83ad191db6c13e0ca04b5acf06ef69c4013378580df98d634ee4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
