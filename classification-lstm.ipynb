{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim==4.1.2\n",
    "!pip install scikit-learn==0.24.2\n",
    "!pip install seaborn==0.11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = '/tf/sa-experiments/corpus'\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    !tar xvzf corpus.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "import shutil, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "USE_EMBEDDING = True\n",
    "EMBEDDING_TYPE = 'skip'\n",
    "\n",
    "\n",
    "if USE_EMBEDDING:\n",
    "    embeddings_index = KeyedVectors.load_word2vec_format(f'{base_dir}/embeddings/{EMBEDDING_TYPE}_s{EMBEDDING_DIM}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = 745307\n",
    "validation_samples = 82811\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = 1\n",
    "subset_training = round((proportion * training_samples) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "corpus_dir = '/tf/sa-experiments/corpus/reviews'\n",
    "\n",
    "dataset_training = (preprocessing\n",
    "    .text_dataset_from_directory(corpus_dir,\n",
    "                                 validation_split=0.1,\n",
    "                                 subset='training',\n",
    "                                 shuffle=True,\n",
    "                                 batch_size=batch_size,\n",
    "                                 seed=seed)\n",
    ")\n",
    "\n",
    "class_names = dataset_training.class_names\n",
    "\n",
    "dataset_training = (dataset_training\n",
    "    .map(lambda x, y: (x, tf.one_hot(y, depth=3)))\n",
    "    .take(subset_training))\n",
    "\n",
    "dataset_validation = (preprocessing\n",
    "    .text_dataset_from_directory(\n",
    "        corpus_dir,\n",
    "        validation_split=0.1,\n",
    "        subset='validation',\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        seed=seed)\n",
    "    .map(lambda x, y: (x, tf.one_hot(y, depth=3)))\n",
    ")\n",
    "\n",
    "num_samples = np.concatenate([x \n",
    "                              for x, _ \n",
    "                              in dataset_training.as_numpy_iterator()]).shape[0]\n",
    "\n",
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 200_000\n",
    "SEQUENCE_LENGTH = 1000\n",
    "\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    "    max_tokens=VOCAB_SIZE\n",
    "    )\n",
    "encoder.adapt(dataset_training.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_EMBEDDING:\n",
    "    voc = encoder.get_vocabulary()\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "    num_tokens = len(voc)\n",
    "    embedding_matrix = np.zeros((num_tokens, EMBEDDING_DIM))\n",
    "\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        \n",
    "        if embeddings_index.has_index_for(word):\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "            hits+=1\n",
    "        else:\n",
    "            misses+=1\n",
    "\n",
    "    print(f\"Hits: {hits}\")\n",
    "    print(f\"Misses: {misses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = tf.keras.layers.Embedding(mask_zero=True, \n",
    "                                input_dim=len(encoder.get_vocabulary()),\n",
    "                                output_dim=EMBEDDING_DIM, \n",
    "                                trainable=True)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    emb,\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300, return_sequences=True, stateful=False)),\n",
    "    tf.keras.layers.Dense(SEQUENCE_LENGTH, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(.6),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "emb.set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = f'/tf/sa-experiments/logs/lstm_{num_samples}_{EMBEDDING_DIM}_{VOCAB_SIZE}_{SEQUENCE_LENGTH}_{USE_EMBEDDING}_{EMBEDDING_TYPE}'\n",
    "\n",
    "model_dir = f'{base_dir}/model'\n",
    "history_dir = f'{base_dir}/fit_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_directory(directory: str, recreate : bool = True):\n",
    "\n",
    "    shutil.rmtree(directory, ignore_errors=True)\n",
    "    os.makedirs(directory, exist_ok=True) if recreate else None\n",
    "\n",
    "clean_directory(base_dir)\n",
    "clean_directory(history_dir)\n",
    "clean_directory(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath= model_dir + '/{epoch:02d}.tf')\n",
    "csv_logger_callback = tf.keras.callbacks.CSVLogger(f'{history_dir}/training.log')\n",
    "\n",
    "history = model.fit(dataset_training,\n",
    "                    validation_data=dataset_validation, \n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[model_checkpoint_callback, csv_logger_callback],\n",
    "                    epochs=5\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "def plot_results(clf, X_validation, Y_validation):\n",
    "\n",
    "    Y_predict = np.argmax(clf.predict(X_validation), axis=1)\n",
    "    fig, axs = plt.subplots(figsize=(15, 20), nrows=2, ncols=1)\n",
    "\n",
    "    map_classes = np.vectorize(lambda x: class_names[x])\n",
    "    \n",
    "    sns.heatmap(pd.DataFrame(confusion_matrix(map_classes(Y_predict), \n",
    "                                              map_classes(Y_validation), \n",
    "                                              normalize='true', \n",
    "                                              labels=class_names), \n",
    "                             columns=class_names, \n",
    "                             index=class_names), \n",
    "                annot=True,\n",
    "                cmap='Blues',\n",
    "                ax=axs[0])\n",
    "    \n",
    "    sns.heatmap(pd.DataFrame(confusion_matrix(map_classes(Y_predict), \n",
    "                                              map_classes(Y_validation),\n",
    "                                              labels=class_names), \n",
    "                             columns=class_names, \n",
    "                             index=class_names), \n",
    "                annot=True,\n",
    "                cmap='Blues',\n",
    "                ax=axs[1])\n",
    "\n",
    "    axs[0].set_title(f'{clf.__class__.__name__} | Accuracy : {round(accuracy_score(Y_validation, Y_predict), 2)}')\n",
    "    axs[1].set_title(f'{clf.__class__.__name__} | Accuracy : {accuracy_score(Y_validation, Y_predict, normalize=False)}')\n",
    "\n",
    "    fig.savefig(f'{base_dir}/LSTM.svg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log = pd.read_csv(f'{history_dir}/training.log')\n",
    "\n",
    "epoch = (training_log\n",
    "    .sort_values(by=['val_loss'])\n",
    "    .reset_index()\n",
    "    ['epoch'][0] + 1)\n",
    "\n",
    "final_dataset_validation = np.concatenate([\n",
    "    np.concatenate([x.reshape(x.shape[0], 1),y], axis=1)\n",
    "    for x, y in dataset_validation.as_numpy_iterator()])\n",
    "\n",
    "model.load_weights(model_dir + f\"/{str(epoch).zfill(2)}.tf\")\n",
    "Y_validation = np.argmax(final_dataset_validation[:,1:], axis=1)\n",
    "\n",
    "plot_results(model, final_dataset_validation[:,0], Y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 20), nrows=2, ncols=1)\n",
    "\n",
    "sns.lineplot(data=training_log\n",
    "                    .melt(value_vars=['categorical_accuracy', 'val_categorical_accuracy'],\n",
    "                          id_vars=['epoch']), \n",
    "             x='epoch',\n",
    "             y='value',\n",
    "             hue='variable',\n",
    "             ax=ax[0])\n",
    "\n",
    "ax[0].set_xticks(range(0,5))\n",
    "\n",
    "sns.lineplot(data=training_log\n",
    "                    .melt(value_vars=['loss', 'val_loss'],\n",
    "                          id_vars=['epoch']), \n",
    "             x='epoch',\n",
    "             y='value',\n",
    "             hue='variable',\n",
    "             ax=ax[1])\n",
    "ax[1].set_xticks(range(0,5))\n",
    "\n",
    "fig.savefig(f'{base_dir}/training.svg', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7d30c5eb6ce83ad191db6c13e0ca04b5acf06ef69c4013378580df98d634ee4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
